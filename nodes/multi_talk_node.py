# IndexTTS2 Multi-Talk Node with Emotion Control
# IndexTTS2 å¤šäººå¯¹è¯è¯­éŸ³åˆæˆèŠ‚ç‚¹ï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰

import os
import torch
import numpy as np
import tempfile
import torchaudio
from typing import Optional, Tuple, Any, List, Dict
import folder_paths
import torch.nn.functional as F
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# å¯¼å…¥é«˜çº§éŸ³é¢‘ç³»ç»Ÿ
try:
    from advanced_audio_systems import SpeakerEmbeddingCache, VoiceConsistencyController, AdaptiveQualityMonitor
    ADVANCED_SYSTEMS_AVAILABLE = True
except ImportError as e:
    print(f"[MultiTalkNode] é«˜çº§éŸ³é¢‘ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")
    ADVANCED_SYSTEMS_AVAILABLE = False

# æ™ºèƒ½éŸ³é¢‘é¢„å¤„ç†å™¨
class IntelligentAudioPreprocessor:
    """æ™ºèƒ½éŸ³é¢‘é¢„å¤„ç†å™¨"""

    def __init__(self, sample_rate: int = 22050):
        self.sample_rate = sample_rate
        self.noise_gate_threshold = -40  # dB
        self.compressor_threshold = -12  # dB
        self.compressor_ratio = 4.0

    def apply_noise_gate(self, audio: torch.Tensor, threshold_db: float = -40) -> torch.Tensor:
        """å™ªå£°é—¨é™å¤„ç†"""
        # è½¬æ¢ä¸ºdB
        audio_db = 20 * torch.log10(torch.abs(audio) + 1e-8)

        # åˆ›å»ºé—¨é™æ©ç 
        gate_mask = audio_db > threshold_db

        # åº”ç”¨è½¯é—¨é™ï¼ˆé¿å…çªå˜ï¼‰
        soft_mask = torch.sigmoid((audio_db - threshold_db) * 0.5)

        return audio * soft_mask

    def apply_dynamic_compression(self, audio: torch.Tensor,
                                threshold_db: float = -12,
                                ratio: float = 4.0) -> torch.Tensor:
        """åŠ¨æ€èŒƒå›´å‹ç¼©"""
        # è®¡ç®—éŸ³é¢‘åŒ…ç»œ
        envelope = torch.abs(audio)
        envelope_db = 20 * torch.log10(envelope + 1e-8)

        # è®¡ç®—å¢ç›Šå‡å°‘
        gain_reduction = torch.zeros_like(envelope_db)
        over_threshold = envelope_db > threshold_db
        gain_reduction[over_threshold] = (envelope_db[over_threshold] - threshold_db) * (1 - 1/ratio)

        # åº”ç”¨å¢ç›Šå‡å°‘
        gain_linear = torch.pow(10, -gain_reduction / 20)

        return audio * gain_linear

    def apply_spectral_enhancement(self, audio: torch.Tensor,
                                 enhancement_strength: float = 0.3) -> torch.Tensor:
        """é¢‘è°±å¢å¼º"""
        if audio.shape[-1] < 1024:
            return audio

        # é«˜é¢‘å¢å¼ºæ»¤æ³¢å™¨
        kernel = torch.tensor([[-0.05, -0.1, 0.7, -0.1, -0.05]], dtype=audio.dtype, device=audio.device)
        kernel = kernel.unsqueeze(0)

        enhanced_channels = []
        for ch in range(audio.shape[0]):
            ch_data = audio[ch:ch+1].unsqueeze(0)
            enhanced = F.conv1d(ch_data, kernel, padding=2)

            # æ··åˆåŸå§‹å’Œå¢å¼ºä¿¡å·
            mixed = ch_data * (1 - enhancement_strength) + enhanced * enhancement_strength
            enhanced_channels.append(mixed.squeeze(0))

        return torch.cat(enhanced_channels, dim=0)

    def normalize_loudness(self, audio: torch.Tensor, target_lufs: float = -23.0) -> torch.Tensor:
        """å“åº¦æ ‡å‡†åŒ–ï¼ˆç®€åŒ–ç‰ˆLUFSï¼‰"""
        # è®¡ç®—RMS
        rms = torch.sqrt(torch.mean(audio ** 2))

        if rms > 0:
            # ç®€åŒ–çš„LUFSåˆ°RMSè½¬æ¢
            target_rms = 10 ** ((target_lufs + 3.01) / 20)  # è¿‘ä¼¼è½¬æ¢
            gain = target_rms / rms

            # é™åˆ¶å¢ç›ŠèŒƒå›´
            gain = torch.clamp(gain, 0.1, 3.0)
            audio = audio * gain

        return audio

    def process(self, audio: torch.Tensor,
                noise_gate: bool = True,
                compression: bool = True,
                spectral_enhancement: bool = True,
                loudness_normalization: bool = True) -> torch.Tensor:
        """å®Œæ•´çš„éŸ³é¢‘é¢„å¤„ç†æµç¨‹"""
        processed_audio = audio.clone()

        if noise_gate:
            processed_audio = self.apply_noise_gate(processed_audio)

        if compression:
            processed_audio = self.apply_dynamic_compression(processed_audio)

        if spectral_enhancement:
            processed_audio = self.apply_spectral_enhancement(processed_audio)

        if loudness_normalization:
            processed_audio = self.normalize_loudness(processed_audio)

        # æœ€ç»ˆé™å¹…
        processed_audio = torch.clamp(processed_audio, -0.95, 0.95)

        return processed_audio

class IndexTTS2MultiTalkNode:
    """
    IndexTTS2 å¤šäººå¯¹è¯è¯­éŸ³åˆæˆèŠ‚ç‚¹ï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰
    Multi-speaker conversation text-to-speech synthesis node for IndexTTS2 with emotion control
    """

    def __init__(self):
        self.model = None
        self.model_config = None
        # æ™ºèƒ½éŸ³é¢‘é¢„å¤„ç†å™¨
        self.audio_preprocessor = IntelligentAudioPreprocessor()

        # é«˜çº§éŸ³é¢‘ç³»ç»Ÿï¼ˆç¬¬äºŒé˜¶æ®µæ”¹è¿›ï¼‰
        if ADVANCED_SYSTEMS_AVAILABLE:
            self.speaker_embedding_cache = SpeakerEmbeddingCache(
                cache_size=100,  # å¤šäººå¯¹è¯èŠ‚ç‚¹ä½¿ç”¨è¾ƒå°çš„ç¼“å­˜
                similarity_threshold=0.92,
                enable_multi_sample_fusion=True
            )
            self.voice_consistency_controller = VoiceConsistencyController(
                consistency_threshold=0.75,  # å¤šäººå¯¹è¯å…è®¸æ›´å¤šå˜åŒ–
                adaptation_rate=0.15
            )
            self.quality_monitor = AdaptiveQualityMonitor()
            print("[MultiTalkNode] âœ“ é«˜çº§éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        else:
            self.speaker_embedding_cache = None
            self.voice_consistency_controller = None
            self.quality_monitor = None
            print("[MultiTalkNode] âš ï¸ é«˜çº§éŸ³é¢‘ç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åŠŸèƒ½")
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "num_speakers": (["1", "2", "3", "4"], {
                    "default": "2",
                    "tooltip": "å¯¹è¯äººæ•° / Number of speakers (1=çº¯è¯­éŸ³å…‹éš†)"
                }),
                "conversation_text": ("STRING", {
                    "multiline": True,
                    "default": "Speaker1: Hello, how are you today!\nSpeaker2: I'm doing great, thank you for asking!",
                    "placeholder": "å•äººæ¨¡å¼ï¼šç›´æ¥è¾“å…¥æ–‡æœ¬\\nå¤šäººæ¨¡å¼ï¼šSpeaker1: æ–‡æœ¬\\nSpeaker2: æ–‡æœ¬..."
                }),
                "speaker1_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº1çš„éŸ³é¢‘æ ·æœ¬ / Speaker 1 audio sample"
                }),
                "output_filename": ("STRING", {
                    "default": "multi_talk_emotion_output.wav",
                    "placeholder": "è¾“å‡ºéŸ³é¢‘æ–‡ä»¶å"
                }),
            },
            "optional": {
                "speaker2_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº2çš„éŸ³é¢‘æ ·æœ¬ / Speaker 2 audio sample (å¤šäººæ¨¡å¼å¿…éœ€)"
                }),
                "speaker3_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº3çš„éŸ³é¢‘æ ·æœ¬ï¼ˆ3-4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 3 audio sample (required for 3-4 speakers)"
                }),
                "speaker4_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº4çš„éŸ³é¢‘æ ·æœ¬ï¼ˆ4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 4 audio sample (required for 4 speakers)"
                }),
                "model_manager": ("INDEXTTS2_MODEL",),
                "language": (["auto", "zh", "en", "zh-en"], {
                    "default": "auto"
                }),
                "speed": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.5,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯­é€Ÿæ§åˆ¶ / Speed control"
                }),
                "silence_duration": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 3.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "å…¨å±€é»˜è®¤é™éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰/ Global default silence duration between speakers (seconds)"
                }),
                "speaker1_pause": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 5.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº1è¯´å®Œåçš„åœé¡¿æ—¶é—´ï¼ˆç§’ï¼‰/ Pause duration after Speaker 1 (seconds)"
                }),
                "speaker2_pause": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 5.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº2è¯´å®Œåçš„åœé¡¿æ—¶é—´ï¼ˆç§’ï¼‰/ Pause duration after Speaker 2 (seconds)"
                }),
                "speaker3_pause": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 5.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº3è¯´å®Œåçš„åœé¡¿æ—¶é—´ï¼ˆç§’ï¼‰/ Pause duration after Speaker 3 (seconds)"
                }),
                "speaker4_pause": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 5.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº4è¯´å®Œåçš„åœé¡¿æ—¶é—´ï¼ˆç§’ï¼‰/ Pause duration after Speaker 4 (seconds)"
                }),
                "voice_consistency": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "å£°éŸ³ä¸€è‡´æ€§å¼ºåº¦ï¼ˆè¶Šé«˜è¶Šæ¥è¿‘å‚è€ƒéŸ³é¢‘ï¼‰/ Voice consistency strength (higher = closer to reference audio)"
                }),
                "reference_boost": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨å‚è€ƒéŸ³é¢‘å¢å¼ºï¼ˆæé«˜å£°éŸ³ç›¸ä¼¼åº¦ï¼‰/ Enable reference audio enhancement (improves voice similarity)"
                }),
                # æƒ…æ„Ÿæ§åˆ¶è¾“å…¥ç«¯å£
                "speaker1_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº1æƒ…æ„Ÿé…ç½® / Speaker 1 emotion config"
                }),
                "speaker2_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº2æƒ…æ„Ÿé…ç½® / Speaker 2 emotion config"
                }),
                "speaker3_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº3æƒ…æ„Ÿé…ç½®ï¼ˆ3-4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 3 emotion config (required for 3-4 speakers)"
                }),
                "speaker4_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº4æƒ…æ„Ÿé…ç½®ï¼ˆ4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 4 emotion config (required for 4 speakers)"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 1.5,
                    "step": 0.1,
                    "display": "slider"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "slider"
                }),
                "use_fp16": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "ä½¿ç”¨åŠç²¾åº¦æ¨ç†ï¼ˆé™ä½å†…å­˜ä½¿ç”¨ï¼‰/ Use FP16 inference (reduces memory usage)"
                }),
                "use_cuda_kernel": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "ä½¿ç”¨CUDAå†…æ ¸åŠ é€Ÿ / Use CUDA kernel acceleration"
                }),
                "use_accel": ("BOOLEAN", {
                    "default": True,
                    "label_on": "å¯ç”¨",
                    "label_off": "ç¦ç”¨",
                    "tooltip": "âš¡ å¯ç”¨GPT2åŠ é€Ÿå¼•æ“ï¼ˆæ¨èï¼Œæå‡30-50%é€Ÿåº¦ï¼‰/ Enable GPT2 acceleration engine (recommended, 30-50% faster)"
                }),
                "use_torch_compile": ("BOOLEAN", {
                    "default": False,
                    "label_on": "å¯ç”¨",
                    "label_off": "ç¦ç”¨",
                    "tooltip": "ğŸš€ å¯ç”¨torch.compileä¼˜åŒ–ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼Œåç»­åŠ é€Ÿï¼Œé€‚åˆæ‰¹é‡å¤„ç†ï¼‰/ Enable torch.compile optimization (slow first time, faster afterwards, good for batch processing)"
                }),
                "verbose": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¾ç¤ºè¯¦ç»†æ—¥å¿— / Show verbose logs"
                }),
            }
        }
    
    RETURN_TYPES = ("AUDIO", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("audio", "output_path", "info", "emotion_analysis")
    FUNCTION = "synthesize_conversation"
    CATEGORY = "IndexTTS2/Advanced"
    DESCRIPTION = "1-4 speaker conversation synthesis with individual emotion control using IndexTTS2 (1=voice cloning, 2-4=conversation)"
    
    def synthesize_conversation(
        self,
        num_speakers: str,
        conversation_text: str,
        speaker1_audio: dict,
        output_filename: str,
        speaker2_audio: Optional[dict] = None,
        speaker3_audio: Optional[dict] = None,
        speaker4_audio: Optional[dict] = None,
        model_manager: Optional[Any] = None,
        language: str = "auto",
        speed: float = 1.0,
        silence_duration: float = 0.5,
        speaker1_pause: float = 0.5,
        speaker2_pause: float = 0.5,
        speaker3_pause: float = 0.5,
        speaker4_pause: float = 0.5,
        voice_consistency: float = 1.0,
        reference_boost: bool = True,
        speaker1_emotion_config: Optional[dict] = None,
        speaker2_emotion_config: Optional[dict] = None,
        speaker3_emotion_config: Optional[dict] = None,
        speaker4_emotion_config: Optional[dict] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        use_fp16: bool = False,
        use_cuda_kernel: bool = False,
        use_accel: bool = True,
        use_torch_compile: bool = False,
        verbose: bool = True
    ) -> Tuple[dict, str, str, str]:
        """
        æ‰§è¡Œå¤šäººå¯¹è¯è¯­éŸ³åˆæˆï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰
        Perform multi-speaker conversation text-to-speech synthesis with emotion control
        """
        try:
            # éªŒè¯è¾“å…¥
            if not conversation_text.strip():
                raise ValueError("Conversation text cannot be empty")

            num_speakers_int = int(num_speakers)

            # æ£€æŸ¥æ˜¯å¦ä¸ºå•äººæ¨¡å¼
            if num_speakers_int == 1:
                # å•äººæ¨¡å¼ï¼šçº¯è¯­éŸ³å…‹éš†
                # FIX: Passed ALL missing parameters to single speaker function
                return self._synthesize_single_speaker(
                    conversation_text, speaker1_audio, speaker1_emotion_config,
                    output_filename, model_manager, language, speed, temperature,
                    top_p, use_fp16, use_cuda_kernel, verbose, use_accel, use_torch_compile
                )

            # å¤šäººæ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
            # éªŒè¯è¯´è¯äººéŸ³é¢‘
            if speaker2_audio is None:
                raise ValueError("Speaker 2 audio is required for 2+ speakers conversation")

            speaker_audios = [speaker1_audio, speaker2_audio]
            if num_speakers_int >= 3:
                if speaker3_audio is None:
                    raise ValueError("Speaker 3 audio is required for 3+ speakers conversation")
                speaker_audios.append(speaker3_audio)
            if num_speakers_int >= 4:
                if speaker4_audio is None:
                    raise ValueError("Speaker 4 audio is required for 4 speakers conversation")
                speaker_audios.append(speaker4_audio)

            # --- FIX START: Extract Custom Speaker Names from Config ---
            emotion_inputs = [speaker1_emotion_config, speaker2_emotion_config, speaker3_emotion_config, speaker4_emotion_config]
            custom_speaker_names = []
            
            for i in range(num_speakers_int):
                name = f"Speaker{i+1}" # Default name
                if i < len(emotion_inputs) and emotion_inputs[i] is not None:
                    if "speaker_name" in emotion_inputs[i]:
                        provided_name = emotion_inputs[i]["speaker_name"]
                        if provided_name and provided_name.strip():
                            name = provided_name.strip()
                custom_speaker_names.append(name)
                
            if verbose:
                print(f"[MultiTalk] Configured Speakers: {custom_speaker_names}")
            # --- FIX END ---

            # è§£æå¯¹è¯æ–‡æœ¬
            conversation_lines = self._parse_conversation(conversation_text, num_speakers_int, verbose, custom_speaker_names)

            # å‡†å¤‡è¯´è¯äººé…ç½®
            speaker_configs = self._prepare_speaker_configs(
                num_speakers_int,
                speaker_audios,
                emotion_inputs,
                verbose,
                voice_consistency,
                reference_boost,
                custom_speaker_names
            )

            # è·å–æ¨¡å‹å®ä¾‹
            if model_manager is not None:
                model = model_manager
            else:
                model = self._load_default_model(use_fp16, use_cuda_kernel, use_accel, use_torch_compile)
            
            # åˆæˆæ¯ä¸ªå¯¹è¯ç‰‡æ®µï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰
            audio_segments = []
            emotion_analysis_list = []

            for line_info in conversation_lines:
                speaker_name = line_info["speaker_name"]
                text = line_info["text"]

                # ä½¿ç”¨è¯´è¯äººåç§°æŸ¥æ‰¾é…ç½®
                if speaker_name not in speaker_configs:
                    if verbose:
                        print(f"[MultiTalk] Warning: Speaker '{speaker_name}' not configured, skipping...")
                    continue

                speaker_config = speaker_configs[speaker_name]
                speaker_audio_path = speaker_config["audio_path"]
                emotion_config = speaker_config["emotion_config"]

                if verbose:
                    print(f"[MultiTalk] Synthesizing {speaker_name}: {text[:50]}...")
                    if emotion_config["mode"] != "none":
                        print(f"[MultiTalk] Emotion mode: {emotion_config['mode']}")

                # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                    temp_output = tmp_file.name

                # å¤„ç†è¯­è¨€å‚æ•°
                processed_language = language if language != "auto" else "zh"

                # æ‰§è¡Œå•ä¸ªç‰‡æ®µçš„æƒ…æ„Ÿåˆæˆï¼ˆå¸¦ä¸€è‡´æ€§æ§åˆ¶ï¼‰
                # === FIX: ADDED SPEED PARAMETER HERE ===
                emotion_analysis = self._synthesize_with_emotion(
                    model, text, speaker_audio_path, emotion_config,
                    temp_output, temperature, top_p, verbose, voice_consistency, 
                    processed_language, speed=speed
                )
                emotion_analysis_list.append(f"{speaker_name}: {emotion_analysis}")

                # åŠ è½½åˆæˆçš„éŸ³é¢‘
                segment_audio = self._load_audio(temp_output)
                audio_segments.append(segment_audio)

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_output)
                except:
                    pass
            
            # å‡†å¤‡ä¸ªæ€§åŒ–åœé¡¿æ—¶é—´é…ç½®ï¼ˆä½¿ç”¨å­—å…¸ï¼‰
            speaker_pauses = {}
            default_pauses = [speaker1_pause, speaker2_pause, speaker3_pause, speaker4_pause]
            for i, name in enumerate(custom_speaker_names):
                if i < len(default_pauses):
                    speaker_pauses[name] = default_pauses[i]

            # åˆå¹¶éŸ³é¢‘ç‰‡æ®µï¼ˆä½¿ç”¨ä¸ªæ€§åŒ–åœé¡¿æ—¶é—´ï¼‰
            final_audio = self._merge_audio_segments_with_custom_pauses(
                audio_segments, conversation_lines, speaker_pauses, silence_duration, verbose
            )
            
            # å‡†å¤‡è¾“å‡ºè·¯å¾„
            output_dir = folder_paths.get_output_directory()
            output_path = os.path.join(output_dir, output_filename)
            
            # ä¿å­˜æœ€ç»ˆéŸ³é¢‘
            torchaudio.save(output_path, final_audio["waveform"], final_audio["sample_rate"])

            # è´¨é‡ç›‘æ§ï¼ˆç¬¬äºŒé˜¶æ®µæ”¹è¿›ï¼‰
            if self.quality_monitor is not None:
                try:
                    # å¯¹æœ€ç»ˆéŸ³é¢‘è¿›è¡Œè´¨é‡è¯„ä¼°
                    quality_assessment = self.quality_monitor.assess_quality(
                        final_audio["waveform"].float(), final_audio["sample_rate"]
                    )

                    if verbose:
                        print(f"[MultiTalk] ğŸµ å¤šäººå¯¹è¯éŸ³é¢‘è´¨é‡è¯„ä¼°:")
                        print(f"  - ç»¼åˆè´¨é‡åˆ†æ•°: {quality_assessment['overall_quality']:.3f}")
                        print(f"  - SNR: {quality_assessment['metrics']['snr']:.1f} dB")
                        print(f"  - THD: {quality_assessment['metrics']['thd']:.3f}")
                        print(f"  - åŠ¨æ€èŒƒå›´: {quality_assessment['metrics']['dynamic_range']:.1f} dB")
                        print(f"  - å³°å€¼ç”µå¹³: {quality_assessment['metrics']['peak_level']:.1f} dB")

                        if quality_assessment['violations'] > 0:
                            print(f"  âš ï¸  æ£€æµ‹åˆ° {quality_assessment['violations']} é¡¹è´¨é‡é—®é¢˜")
                            print(f"  â„¹ï¸ è‡ªåŠ¨æ”¹è¿›åŠŸèƒ½å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘")
                        else:
                            print(f"  âœ… å¤šäººå¯¹è¯éŸ³é¢‘è´¨é‡è‰¯å¥½")

                except Exception as e:
                    if verbose:
                        print(f"[MultiTalk] âš ï¸ è´¨é‡ç›‘æ§å¤±è´¥: {e}")

            # ç¡®ä¿éŸ³é¢‘æ ¼å¼å…¼å®¹ComfyUI
            waveform = final_audio["waveform"]
            sample_rate = final_audio["sample_rate"]
            
            # åº”ç”¨ComfyUIå…¼å®¹æ€§æ£€æŸ¥
            from .audio_utils import fix_comfyui_audio_compatibility
            waveform = fix_comfyui_audio_compatibility(waveform)
            
            # ComfyUI AUDIOæ ¼å¼éœ€è¦ [batch, channels, samples]
            if waveform.dim() == 1:
                # [samples] -> [1, 1, samples]
                waveform = waveform.unsqueeze(0).unsqueeze(0)
            elif waveform.dim() == 2:
                # [channels, samples] -> [1, channels, samples]
                waveform = waveform.unsqueeze(0)
            
            # åˆ›å»ºComfyUI AUDIOæ ¼å¼
            comfyui_audio = {
                "waveform": waveform,
                "sample_rate": sample_rate
            }
            
            if verbose:
                print(f"[MultiTalk] å¯¹è¯åˆæˆå®Œæˆ: {len(conversation_lines)} ä¸ªç‰‡æ®µ")
                print(f"[MultiTalk] æœ€ç»ˆéŸ³é¢‘æ ¼å¼: {waveform.shape}, é‡‡æ ·ç‡: {sample_rate}")

            # ç”Ÿæˆä¿¡æ¯å­—ç¬¦ä¸²ï¼ˆåŒ…å«ä¸ªæ€§åŒ–åœé¡¿æ—¶é—´ï¼‰
            info = self._generate_info_with_emotion_and_pauses(
                conversation_lines, num_speakers_int, output_path, language, speed,
                silence_duration, speaker_pauses, speaker_configs
            )

            # ç”Ÿæˆæƒ…æ„Ÿåˆ†æå­—ç¬¦ä¸²
            emotion_analysis = "\n".join(emotion_analysis_list)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for speaker_name, config in speaker_configs.items():
                try:
                    os.unlink(config["audio_path"])
                except:
                    pass

            return (comfyui_audio, output_path, info, emotion_analysis)
            
        except Exception as e:
            error_msg = f"IndexTTS2 multi-talk synthesis failed: {str(e)}"
            print(f"[MultiTalk Error] {error_msg}")
            raise RuntimeError(error_msg)

    def _synthesize_single_speaker(
        self,
        text: str,
        speaker_audio: dict,
        emotion_config: Optional[dict],
        output_filename: str,
        model_manager: Optional[Any],
        language: str,
        speed: float,
        temperature: float,
        top_p: float,
        use_fp16: bool,
        use_cuda_kernel: bool,
        verbose: bool,
        use_accel: bool = True, # FIX: Added arg
        use_torch_compile: bool = False # FIX: Added arg
    ) -> Tuple[dict, str, str, str]:
        """
        å•äººæ¨¡å¼åˆæˆ
        Single speaker mode synthesis
        """
        try:
            if verbose:
                print(f"[MultiTalk] å•äººæ¨¡å¼åˆæˆ / Single speaker mode synthesis")
                print(f"[MultiTalk] æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦ / Text length: {len(text)} characters")

            # è·å–æ¨¡å‹å®ä¾‹
            if model_manager is not None:
                model = model_manager
            else:
                # FIX: Now arguments are available
                model = self._load_default_model(use_fp16, use_cuda_kernel, use_accel, use_torch_compile)

            # å‡†å¤‡è¯´è¯äººéŸ³é¢‘
            speaker_audio_path = self._prepare_speaker_audios([speaker_audio], verbose, 1.0, True)[0]

            # å‡†å¤‡æƒ…æ„Ÿæ§åˆ¶å‚æ•°
            if emotion_config is None:
                emotion_config = {"mode": "none"}

            # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                temp_output_path = temp_file.name

            # å¤„ç†è¯­è¨€å‚æ•°
            processed_language = language if language != "auto" else "zh"

            # åˆæˆéŸ³é¢‘
            # === FIX: ADDED SPEED PARAMETER HERE ===
            emotion_analysis = self._synthesize_with_emotion(
                model, text, speaker_audio_path, emotion_config, temp_output_path,
                temperature, top_p, verbose, language=processed_language, speed=speed
            )

            # ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½éŸ³é¢‘
            if os.path.exists(temp_output_path):
                audio_tensor, sample_rate = torchaudio.load(temp_output_path)
                # ç¡®ä¿é‡‡æ ·ç‡æ­£ç¡®
                if sample_rate != 24000:
                    import torchaudio.transforms as T
                    resampler = T.Resample(sample_rate, 24000)
                    audio_tensor = resampler(audio_tensor)
            else:
                raise RuntimeError("åˆæˆå¤±è´¥ï¼Œä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_output_path)
            except:
                pass

            # ä¿å­˜éŸ³é¢‘
            output_path = self._save_audio(audio_tensor, output_filename)

            # ç”Ÿæˆä¿¡æ¯
            info = f"å•äººè¯­éŸ³åˆæˆå®Œæˆ\næ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦\néŸ³é¢‘é•¿åº¦: {len(audio_tensor[0])/24000:.2f} ç§’"
            if emotion_config["mode"] != "none":
                info += f"\næƒ…ç»ªæ¨¡å¼: {emotion_config['mode']}"
                info += f"\næƒ…æ„Ÿåˆ†æ: {emotion_analysis}"

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(speaker_audio_path)
            except:
                pass

            # ç¡®ä¿éŸ³é¢‘æ ¼å¼ç¬¦åˆ ComfyUI æ ‡å‡† [batch, channels, samples]
            if audio_tensor.dim() == 1:
                # [samples] -> [1, 1, samples]
                audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)
            elif audio_tensor.dim() == 2:
                # [channels, samples] -> [1, channels, samples]
                audio_tensor = audio_tensor.unsqueeze(0)

            if verbose:
                print(f"[MultiTalk SingleSpeaker] æœ€ç»ˆéŸ³é¢‘æ ¼å¼: {audio_tensor.shape}")
                print(f"[MultiTalk SingleSpeaker] ComfyUI AUDIOæ ¼å¼: batch={audio_tensor.shape[0]}, channels={audio_tensor.shape[1]}, samples={audio_tensor.shape[2]}")

            # è¿”å›ComfyUIæ ¼å¼çš„éŸ³é¢‘
            comfyui_audio = {"waveform": audio_tensor, "sample_rate": 24000}

            return (comfyui_audio, output_path, info, emotion_analysis)

        except Exception as e:
            error_msg = f"å•äººæ¨¡å¼åˆæˆå¤±è´¥: {str(e)}"
            print(f"[SingleSpeaker Error] {error_msg}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(error_msg)

    def _extract_pause_from_text(self, text: str) -> tuple:
        """ä»æ–‡æœ¬ä¸­æå–åœé¡¿æ—¶é—´æ ‡è®°"""
        import re
        pause_pattern = r'-(\d+(?:\.\d+)?)s-'
        matches = re.findall(pause_pattern, text)
        if matches:
            pause_time = float(matches[-1])
            clean_text = re.sub(pause_pattern, '', text).strip()
            return clean_text, pause_time
        return text, None

    def _parse_conversation(self, conversation_text: str, num_speakers: int, verbose: bool, custom_speaker_names: List[str] = None) -> List[Dict]:
        """è§£æå¯¹è¯æ–‡æœ¬ - æ”¯æŒè‡ªå®šä¹‰è¯´è¯äººåç§°"""
        lines = conversation_text.strip().split('\n')
        conversation_lines = []

        if not custom_speaker_names:
            custom_speaker_names = [f"Speaker{i+1}" for i in range(num_speakers)]

        if verbose:
            print(f"[MultiTalk] è§£æå¯¹è¯æ–‡æœ¬ï¼Œè¯´è¯äººåˆ—è¡¨: {custom_speaker_names}")

        for line in lines:
            line = line.strip()
            if not line:
                continue

            speaker_found = False
            matched_speaker = None
            text_content = ""

            for name in custom_speaker_names:
                if line.startswith(f"{name}:"):
                    matched_speaker = name
                    text_content = line[len(f"{name}:"):].strip()
                    speaker_found = True
                    break
            
            if not speaker_found:
                for i in range(1, num_speakers + 1):
                    target_name = custom_speaker_names[i-1]
                    patterns = [f"Speaker{i}:", f"speaker{i}:", f"Speaker {i}:", f"S{i}:", f"è¯´è¯äºº{i}:"]
                    for pattern in patterns:
                        if line.startswith(pattern):
                            matched_speaker = target_name
                            text_content = line[len(pattern):].strip()
                            speaker_found = True
                            break
                    if speaker_found:
                        break

            if speaker_found and text_content:
                clean_text, pause_time = self._extract_pause_from_text(text_content)
                conversation_lines.append({
                    "speaker_name": matched_speaker,
                    "text": clean_text,
                    "custom_pause": pause_time
                })
            elif line:
                default_speaker = custom_speaker_names[0]
                clean_text, pause_time = self._extract_pause_from_text(line)
                conversation_lines.append({
                    "speaker_name": default_speaker,
                    "text": clean_text,
                    "custom_pause": pause_time
                })
                if verbose:
                    print(f"[MultiTalk] æœªè¯†åˆ«è¯´è¯äººï¼Œé»˜è®¤åˆ†é…ç»™ {default_speaker}: {clean_text[:30]}...")

        if not conversation_lines:
            raise ValueError("No valid conversation lines found.")

        if verbose:
            print(f"[MultiTalk] è§£æåˆ° {len(conversation_lines)} ä¸ªå¯¹è¯ç‰‡æ®µ")

        return conversation_lines

    def _prepare_emotion_configs_from_inputs(self, num_speakers: int,
                                            emotion_config_inputs: List[Optional[Dict]],
                                            verbose: bool) -> List[Dict]:
        """ä»è¾“å…¥çš„æƒ…æ„Ÿé…ç½®å¯¹è±¡å‡†å¤‡æƒ…æ„Ÿæ§åˆ¶é…ç½®"""
        emotion_configs = []

        for i in range(num_speakers):
            emotion_input = emotion_config_inputs[i] if i < len(emotion_config_inputs) else None

            if emotion_input is None or not emotion_input.get("enabled", True):
                emotion_config = {"mode": "none"}
                if verbose:
                    print(f"[MultiTalk] Speaker{i+1}: No emotion control (default)")
            else:
                emotion_config = {
                    "mode": emotion_input.get("mode", "none"),
                    "audio": emotion_input.get("audio", ""),
                    "alpha": emotion_input.get("alpha", 1.0),
                    "vector": emotion_input.get("vector", [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]),
                    "text": emotion_input.get("text", ""),
                    "speaker_name": emotion_input.get("speaker_name", "")
                }
                if verbose:
                    print(f"[MultiTalk] {emotion_input.get('speaker_name', f'Speaker{i+1}')} emotion mode: {emotion_config['mode']}")

            emotion_configs.append(emotion_config)
        return emotion_configs

    def _save_audio(self, audio_tensor, filename):
        """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
        output_dir = folder_paths.get_output_directory()
        if not filename.lower().endswith(('.wav', '.mp3', '.flac')):
            filename = filename + ".wav"
        output_path = os.path.join(output_dir, filename)
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        torchaudio.save(output_path, audio_tensor, 24000)
        print(f"ğŸ’¾ Multi-talk conversation saved to: {output_path}")
        return output_path

    # === FIX: ADDED SPEED PARAMETER HERE ===
    def _synthesize_with_emotion(self, model, text: str, speaker_audio_path: str,
                                emotion_config: Dict, output_path: str,
                                temperature: float, top_p: float, verbose: bool,
                                voice_consistency: float = 1.0, language: str = "zh",
                                speed: float = 1.0) -> str:
        """æ‰§è¡Œå¸¦æƒ…æ„Ÿæ§åˆ¶çš„è¯­éŸ³åˆæˆ"""
        emotion_mode = emotion_config["mode"]
        consistency_temp = max(0.1, temperature / voice_consistency)
        consistency_top_p = min(0.99, top_p * voice_consistency)

        # === FIX: PASSED SPEED TO ALL MODEL.INFER CALLS ===
        if emotion_mode == "none":
            model.infer(
                spk_audio_prompt=speaker_audio_path,
                text=text,
                output_path=output_path,
                verbose=False,
                temperature=consistency_temp,
                top_p=consistency_top_p,
                top_k=50,
                max_text_tokens_per_sentence=60,
                interval_silence=200,
                speed=speed
            )
            return "No emotion control"

        elif emotion_mode == "audio_prompt":
            emotion_audio_info = emotion_config["audio"]
            emotion_alpha = emotion_config["alpha"]

            if emotion_audio_info and isinstance(emotion_audio_info, dict) and "audio_object" in emotion_audio_info:
                emotion_audio_path = self._save_emotion_audio_to_temp(emotion_audio_info["audio_object"])
                if emotion_audio_path:
                    try:
                        model.infer(
                            spk_audio_prompt=speaker_audio_path,
                            text=text,
                            output_path=output_path,
                            emo_audio_prompt=emotion_audio_path,
                            emo_alpha=emotion_alpha,
                            verbose=False,
                            temperature=consistency_temp,
                            top_p=consistency_top_p,
                            top_k=50,
                            max_text_tokens_per_sentence=60,
                            interval_silence=200,
                            speed=speed
                        )
                    finally:
                        try: os.unlink(emotion_audio_path)
                        except: pass
                else:
                    model.infer(spk_audio_prompt=speaker_audio_path, text=text, output_path=output_path, verbose=False, speed=speed)
                return f"Audio emotion ({os.path.basename(emotion_audio)}, Î±={emotion_alpha})"
            else:
                model.infer(spk_audio_prompt=speaker_audio_path, text=text, output_path=output_path, verbose=False, speed=speed)
                return "No emotion audio provided"

        elif emotion_mode == "emotion_vector":
            emotion_vector = emotion_config["vector"]
            max_emotion_value = max(emotion_vector)
            if max_emotion_value == 0.0:
                emotion_vector = emotion_vector.copy()
                emotion_vector[7] = 0.1
            model.infer(
                spk_audio_prompt=speaker_audio_path,
                text=text,
                output_path=output_path,
                emo_vector=emotion_vector,
                verbose=False,
                temperature=consistency_temp,
                top_p=consistency_top_p,
                top_k=50,
                max_text_tokens_per_sentence=60,
                interval_silence=200,
                speed=speed
            )
            return "Vector emotion"

        elif emotion_mode == "text_description":
            emotion_text = emotion_config["text"]
            alpha_val = emotion_config.get("alpha", 1.0)

            if emotion_text.strip():
                model.infer(
                    spk_audio_prompt=speaker_audio_path,
                    text=text,
                    output_path=output_path,
                    use_emo_text=True,
                    emo_text=emotion_text,
                    emo_alpha=alpha_val,
                    verbose=False,
                    temperature=consistency_temp,
                    top_p=consistency_top_p,
                    top_k=50,
                    max_text_tokens_per_sentence=60,
                    interval_silence=200,
                    speed=speed
                )
                return f"Text emotion ({emotion_text[:30]}...)"
            else:
                model.infer(
                    spk_audio_prompt=speaker_audio_path,
                    text=text,
                    output_path=output_path,
                    use_emo_text=True,
                    emo_alpha=alpha_val,
                    verbose=False,
                    temperature=consistency_temp,
                    top_p=consistency_top_p,
                    top_k=50,
                    max_text_tokens_per_sentence=60,
                    interval_silence=200,
                    speed=speed
                )
                return "Text emotion (inferred)"

        else:
            model.infer(spk_audio_prompt=speaker_audio_path, text=text, output_path=output_path, verbose=False, speed=speed)
            return "Auto emotion"

    def _prepare_speaker_audios(self, speaker_audios: List[dict], verbose: bool,
                               voice_consistency: float = 1.0, reference_boost: bool = True) -> List[str]:
        """å‡†å¤‡è¯´è¯äººéŸ³é¢‘æ–‡ä»¶"""
        speaker_audio_paths = []
        for i, speaker_audio in enumerate(speaker_audios):
            if not isinstance(speaker_audio, dict) or "waveform" not in speaker_audio:
                raise ValueError(f"Speaker {i+1} audio must be a ComfyUI AUDIO object")
            waveform = speaker_audio["waveform"]
            sample_rate = speaker_audio["sample_rate"]
            if waveform.dim() == 3: waveform = waveform.squeeze(0)
            if reference_boost and voice_consistency > 1.0:
                waveform = self._enhance_reference_audio(waveform, voice_consistency)
            with tempfile.NamedTemporaryFile(suffix=f"_speaker{i+1}.wav", delete=False) as tmp_file:
                speaker_audio_path = tmp_file.name
            torchaudio.save(speaker_audio_path, waveform, sample_rate)
            speaker_audio_paths.append(speaker_audio_path)
        return speaker_audio_paths

    def _prepare_speaker_configs(self, num_speakers: int, speaker_audios: List[dict],
                                emotion_configs_list: List[Optional[dict]], verbose: bool,
                                voice_consistency: float = 1.0, reference_boost: bool = True,
                                custom_speaker_names: List[str] = None) -> Dict[str, Dict]:
        """å‡†å¤‡è¯´è¯äººé…ç½®"""
        speaker_configs = {}
        if not custom_speaker_names:
            custom_speaker_names = [f"Speaker{i+1}" for i in range(num_speakers)]

        for i in range(num_speakers):
            speaker_name = custom_speaker_names[i]
            speaker_audio = speaker_audios[i]
            if not isinstance(speaker_audio, dict): raise ValueError(f"{speaker_name} audio invalid")

            waveform = speaker_audio["waveform"]
            sample_rate = speaker_audio["sample_rate"]
            if waveform.dim() == 3: waveform = waveform.squeeze(0)
            if reference_boost and voice_consistency > 1.0:
                waveform = self._enhance_reference_audio(waveform, voice_consistency)

            with tempfile.NamedTemporaryFile(suffix=f"_{speaker_name}.wav", delete=False) as tmp_file:
                speaker_audio_path = tmp_file.name
            torchaudio.save(speaker_audio_path, waveform, sample_rate)

            emotion_config = emotion_configs_list[i] if i < len(emotion_configs_list) and emotion_configs_list[i] is not None else {"mode": "none"}
            speaker_configs[speaker_name] = {"audio_path": speaker_audio_path, "emotion_config": emotion_config}
            if verbose: print(f"[MultiTalk] Configured {speaker_name}")

        return speaker_configs

    def _smooth_audio_transition(self, audio1: torch.Tensor, audio2: torch.Tensor, fade_samples: int = 1024) -> torch.Tensor:
        """å¹³æ»‘è¿‡æ¸¡"""
        if audio1.shape[-1] < fade_samples or audio2.shape[-1] < fade_samples:
            return torch.cat([audio1, audio2], dim=-1)
        fade_out = torch.linspace(1.0, 0.0, fade_samples, device=audio1.device, dtype=audio1.dtype)
        fade_in = torch.linspace(0.0, 1.0, fade_samples, device=audio2.device, dtype=audio2.dtype)
        if audio1.dim() == 2:
            fade_out = fade_out.unsqueeze(0).expand(audio1.shape[0], -1)
            fade_in = fade_in.unsqueeze(0).expand(audio2.shape[0], -1)
        audio1_end = audio1[..., -fade_samples:] * fade_out
        audio2_start = audio2[..., :fade_samples] * fade_in
        mixed_section = audio1_end + audio2_start
        result = torch.cat([audio1[..., :-fade_samples], mixed_section, audio2[..., fade_samples:]], dim=-1)
        return result

    def _enhance_reference_audio(self, waveform: torch.Tensor, voice_consistency: float) -> torch.Tensor:
        """å¢å¼ºå‚è€ƒéŸ³é¢‘"""
        try:
            min_length = max(16000, int(0.5 * 22050))
            if waveform.shape[-1] < min_length:
                repeat_times = int(min_length / waveform.shape[-1]) + 1
                waveform = waveform.repeat(1, repeat_times)[:, :min_length]
            if voice_consistency <= 1.0:
                processed_audio = self.audio_preprocessor.normalize_loudness(waveform)
            elif voice_consistency <= 1.5:
                processed_audio = self.audio_preprocessor.process(waveform, noise_gate=True, compression=False, spectral_enhancement=False, loudness_normalization=True)
            else:
                enhancement_strength = min((voice_consistency - 1.0) * 0.3, 0.5)
                processed_audio = self.audio_preprocessor.process(waveform, noise_gate=True, compression=True, spectral_enhancement=True, loudness_normalization=True)
            processed_audio = torch.clamp(processed_audio, -0.95, 0.95)
            return processed_audio
        except Exception as e:
            print(f"[MultiTalk] Audio enhance failed: {e}")
            return waveform

    def _merge_audio_segments_with_custom_pauses(self, audio_segments: List[dict], conversation_lines: List[Dict], speaker_pauses: Dict[str, float], default_silence: float, verbose: bool) -> dict:
        """åˆå¹¶éŸ³é¢‘"""
        if not audio_segments: raise ValueError("No audio segments")
        sample_rate = audio_segments[0]["sample_rate"]
        for i, segment in enumerate(audio_segments):
            if segment["sample_rate"] != sample_rate:
                resampler = torchaudio.transforms.Resample(segment["sample_rate"], sample_rate)
                segment["waveform"] = resampler(segment["waveform"])
                segment["sample_rate"] = sample_rate

        fade_samples = min(512, sample_rate // 50)
        current_waveform = audio_segments[0]["waveform"]
        if current_waveform.dim() == 3: current_waveform = current_waveform.squeeze(0)
        elif current_waveform.dim() == 1: current_waveform = current_waveform.unsqueeze(0)

        for i in range(1, len(audio_segments)):
            next_waveform = audio_segments[i]["waveform"]
            if next_waveform.dim() == 3: next_waveform = next_waveform.squeeze(0)
            elif next_waveform.dim() == 1: next_waveform = next_waveform.unsqueeze(0)

            current_line = conversation_lines[i-1]
            current_speaker_name = current_line["speaker_name"]
            pause_duration = current_line.get("custom_pause") if current_line.get("custom_pause") is not None else speaker_pauses.get(current_speaker_name, default_silence)

            if pause_duration > 0:
                pause_samples = int(pause_duration * sample_rate)
                pause_waveform = torch.zeros(current_waveform.shape[0], pause_samples, device=current_waveform.device, dtype=current_waveform.dtype)
                current_waveform = self._smooth_audio_transition(current_waveform, pause_waveform, fade_samples)
                current_waveform = self._smooth_audio_transition(current_waveform, next_waveform, fade_samples)
            else:
                current_waveform = self._smooth_audio_transition(current_waveform, next_waveform, fade_samples)
        
        return {"waveform": current_waveform, "sample_rate": sample_rate}

    def _load_default_model(self, use_fp16: bool, use_cuda_kernel: bool, use_accel: bool = True, use_torch_compile: bool = False):
        """åŠ è½½é»˜è®¤æ¨¡å‹"""
        try:
            cache_key = f"fp16_{use_fp16}_cuda_{use_cuda_kernel}_accel_{use_accel}_compile_{use_torch_compile}"
            if not hasattr(self, '_model_cache'): self._model_cache = {}
            if cache_key in self._model_cache: return self._model_cache[cache_key]

            from indextts.infer_v2 import IndexTTS2
            from .model_utils import get_indextts2_model_path, validate_model_path
            model_dir, config_path = get_indextts2_model_path()
            validate_model_path(model_dir, config_path)
            
            # Pass use_accel
            model = IndexTTS2(cfg_path=config_path, model_dir=model_dir, use_fp16=use_fp16, use_cuda_kernel=use_cuda_kernel, use_accel=use_accel, use_torch_compile=use_torch_compile)
            self._model_cache[cache_key] = model
            return model
        except Exception as e:
            raise RuntimeError(f"Failed to load IndexTTS2 model: {str(e)}")

    def _load_audio(self, audio_path: str) -> dict:
        from .audio_utils import load_audio_for_comfyui
        return load_audio_for_comfyui(audio_path)

    def _save_emotion_audio_to_temp(self, emotion_audio: dict) -> Optional[str]:
        try:
            import tempfile
            import torchaudio
            if not isinstance(emotion_audio, dict): return None
            waveform = emotion_audio["waveform"]
            if waveform.dim() == 3: waveform = waveform.squeeze(0)
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                emotion_audio_path = tmp_file.name
            torchaudio.save(emotion_audio_path, waveform, emotion_audio["sample_rate"])
            return emotion_audio_path
        except: return None
    
    # Dummy methods for generating info strings (Preserved structure)
    def _generate_info_with_emotion_and_pauses(self, *args, **kwargs): return "Info Generated"
    def _get_qwen_model_info(self): return ["Qwen Info"]
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return float("nan")
